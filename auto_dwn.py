import requests
from bs4 import BeautifulSoup
import feedparser
import re
import logging
import csv
import os
import random
import time
from typing import Optional, Tuple, Dict, List, Any
from tqdm import tqdm
from concurrent.futures import ThreadPoolExecutor, as_completed
from requests.adapters import HTTPAdapter
from urllib3.util.retry import Retry
import functools
import hashlib
import json
import urllib

from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.common.exceptions import TimeoutException, NoSuchElementException


def setup_logging():
    """åˆå§‹åŒ–æ—¥å¿—é…ç½®ï¼Œæ¯æ¬¡è¿è¡Œæ¸…ç©ºæ—¥å¿—æ–‡ä»¶"""
    log_file = 'paper_downloader.log'

    # æ¸…ç©ºæ–‡ä»¶å†…å®¹ï¼ˆå¦‚æœæ–‡ä»¶å­˜åœ¨ï¼‰
    try:
        with open(log_file, 'w', encoding='utf-8') as f:
            pass  # æ‰“å¼€æ–‡ä»¶å¹¶ç«‹å³å…³é—­ä»¥æ¸…ç©ºå†…å®¹
    except Exception as e:
        print(f"âš ï¸ æ— æ³•æ¸…ç©ºæ—¥å¿—æ–‡ä»¶: {str(e)}")

    # é…ç½®æ—¥å¿—ï¼ˆä½¿ç”¨è¿½åŠ æ¨¡å¼ä½†æ–‡ä»¶å·²è¢«æ¸…ç©ºï¼‰
    logging.basicConfig(
        filename=log_file,
        level=logging.INFO,
        format='%(asctime)s - %(levelname)s - %(message)s',
        encoding='gbk',
        force=True
    )

    # æ·»åŠ æ§åˆ¶å°å¤„ç†å™¨
    console = logging.StreamHandler()
    console.setLevel(logging.WARNING)
    formatter = logging.Formatter('%(levelname)s - %(message)s')
    console.setFormatter(formatter)
    logging.getLogger('').addHandler(console)
    logging.info("ğŸ†• ç¨‹åºå¯åŠ¨ï¼Œæ—¥å¿—æ–‡ä»¶å·²æ¸…ç©º")


class SciHubDownloader:
    DEFAULT_CONFIG = [
        {  # é»˜è®¤é…ç½®ç¤ºä¾‹
            "domain": "https://www.sci-hub.ru/",
            "selectors": {
                "input": "#request",
                "search_btn": "#enter > button",
                "download_btn": "#buttons > button",
                "unavailable": "#return > a"
            }
        },
        {  # å¤‡ç”¨é…ç½®
            "domain": "https://www.sci-hub.se/",
            "selectors": {
                "input": "#request",
                "search_btn": "#enter > button",
                "download_btn": "#buttons > button",
                "unavailable": "#return > a"
            }
        }
    ]

    def __init__(self, headless=True):
        self.domain_config = self.DEFAULT_CONFIG
        self.current_domain_idx = 0
        self.driver = self._init_driver(headless)
        self.wait = WebDriverWait(self.driver, 20)

    def _init_driver(self, headless):
        """åˆå§‹åŒ–æµè§ˆå™¨é…ç½®ï¼ˆæ— å¤´æ¨¡å¼ï¼‰"""
        chrome_options = webdriver.ChromeOptions()
        if headless:
            chrome_options.add_argument("--headless=new")
        chrome_options.add_argument("--disable-blink-features=AutomationControlled")
        chrome_options.add_argument("--disable-infobars")
        chrome_options.add_argument("--disable-extensions")
        return webdriver.Chrome(options=chrome_options)

    def _switch_domain(self):
        """åˆ‡æ¢å¤‡ç”¨åŸŸå"""
        self.current_domain_idx = (self.current_domain_idx + 1) % len(self.domain_config)
        logging.debug(f"åˆ‡æ¢åˆ°å¤‡ç”¨åŸŸå: {self.current_domain()}")

    def current_domain(self):
        return self.domain_config[self.current_domain_idx]["domain"]

    def current_selectors(self):
        return self.domain_config[self.current_domain_idx]["selectors"]

    def _parse_pdf_url(self, soup):
        """è§£æPDFé“¾æ¥ï¼ˆä¸åŸæœ‰é€»è¾‘ä¸€è‡´ï¼‰"""
        try:
            if button := soup.find('button', {'id': 'save'}):
                if onclick := button.get('onclick'):
                    return onclick.split("'")[1]
            if iframe := soup.find('iframe') or soup.find('embed'):
                return iframe['src']
            if pdf_link := soup.find('a', href=re.compile(r'.*\.pdf$')):
                return pdf_link['href']
        except Exception:
            return None
        return None

    def fetch_pdf_url(self, title):
        """è¿”å› (PDFé“¾æ¥, é”™è¯¯ä¿¡æ¯)"""
        for _ in range(len(self.domain_config)):
            try:
                # è®¿é—®å½“å‰åŸŸå
                self.driver.get(self.current_domain())

                # æ‰§è¡Œæœç´¢
                input_box = self.wait.until(
                    EC.element_to_be_clickable(
                        (By.CSS_SELECTOR, self.current_selectors()["input"])
                    )
                )
                input_box.clear()
                input_box.send_keys(title)

                self.driver.find_element(
                    By.CSS_SELECTOR, self.current_selectors()["search_btn"]
                ).click()

                # æ£€æŸ¥æ˜¯å¦å¯ç”¨
                try:
                    self.wait.until(
                        EC.presence_of_element_located(
                            (By.CSS_SELECTOR, self.current_selectors()["unavailable"])
                        )
                    )
                    self._switch_domain()
                    continue
                except TimeoutException:
                    pass

                # è§£æPDFé“¾æ¥
                soup = BeautifulSoup(self.driver.page_source, 'html.parser')
                if pdf_url := self._parse_pdf_url(soup):
                    # å¤„ç†ç›¸å¯¹è·¯å¾„
                    if not pdf_url.startswith('http'):
                        base_url = urllib.parse.urlparse(self.current_domain()).scheme + "://" + \
                                   urllib.parse.urlparse(self.current_domain()).netloc
                        pdf_url = urllib.parse.urljoin(base_url, pdf_url)
                    return pdf_url, None

            except Exception as e:
                logging.error(f"Seleniumè¯·æ±‚å¤±è´¥: {str(e)}")
                self._switch_domain()

        return None, "æ‰€æœ‰é•œåƒå°è¯•å¤±è´¥"

    def close(self):
        self.driver.quit()


class RequestCache:
    """ç®€å•çš„è¯·æ±‚ç¼“å­˜ç±»ï¼Œé¿å…å¯¹åŒä¸€èµ„æºé‡å¤è¯·æ±‚"""

    def __init__(self, cache_file='request_cache.json'):
        self.cache_file = cache_file
        self.cache = {}
        self._load_cache()

    def _load_cache(self):
        if os.path.exists(self.cache_file):
            try:
                with open(self.cache_file, 'r', encoding='utf-8') as f:
                    self.cache = json.load(f)
            except:
                self.cache = {}

    def _save_cache(self):
        with open(self.cache_file, 'w', encoding='utf-8') as f:
            json.dump(self.cache, f)

    def get(self, key):
        return self.cache.get(key)

    def set(self, key, value):
        self.cache[key] = value
        # å®šæœŸä¿å­˜ç¼“å­˜
        if len(self.cache) % 10 == 0:
            self._save_cache()

    def __del__(self):
        self._save_cache()


class PaperDownloader:
    def __init__(self, max_workers=5):
        self.max_workers = max_workers
        self.scihub_urls = [
            "https://www.sci-hub.ru/",
            "https://www.sci-hub.se/",
            "https://sci-hub.box/",
            "https://sci-hub.red/",
            "https://sci-hub.al/",
            "https://www.sci-hub.ee/",
            "https://sci-hub.lu/",
            "https://www.sci-hub.ren/",
            "https://sci-hub.shop/",
            "https://sci-hub.vg/"
        ]
        # å°†å·¥ä½œè‰¯å¥½çš„é•œåƒç§»åˆ°å‰é¢
        random.shuffle(self.scihub_urls)

        self.arxiv_api = "http://export.arxiv.org/api/query?search_query=ti:{}"
        self.crossref_api = "https://api.crossref.org/works?query.title={}"
        self.headers = {
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/113.0.0.0 Safari/537.36",
            "Accept-Language": "en-US,en;q=0.9",
            "Referer": "https://scholar.google.com/",
            "DNT": "1"
        }

        # åˆ›å»ºæ›´å¯é çš„session
        self.session = self._create_robust_session()
        self.cache = RequestCache()
        self.active_mirrors = []  # è·Ÿè¸ªå·¥ä½œè‰¯å¥½çš„é•œåƒ

    def _create_robust_session(self):
        """åˆ›å»ºå…·æœ‰è‡ªåŠ¨é‡è¯•åŠŸèƒ½çš„ä¼šè¯"""
        session = requests.Session()
        retry_strategy = Retry(
            total=3,
            backoff_factor=1,
            status_forcelist=[429, 500, 502, 503, 504],
            allowed_methods=["GET", "POST"]
        )
        adapter = HTTPAdapter(max_retries=retry_strategy)
        session.mount("http://", adapter)
        session.mount("https://", adapter)
        session.headers.update(self.headers)
        return session

    def _get_doi_from_title(self, title: str) -> Tuple[Optional[str], Optional[str]]:
        """è¿”å› (DOI, é”™è¯¯ä¿¡æ¯)"""
        # æ£€æŸ¥ç¼“å­˜
        cache_key = f"doi:{hashlib.md5(title.encode()).hexdigest()}"
        cached = self.cache.get(cache_key)
        if cached:
            return cached.get('doi'), cached.get('error')

        try:
            search_url = self.crossref_api.format(title.replace(' ', '+'))
            resp = self.session.get(search_url, timeout=15)
            if resp.status_code == 200:
                data = resp.json()
                if data['message']['items']:
                    doi = data['message']['items'][0]['DOI']
                    self.cache.set(cache_key, {'doi': doi, 'error': None})
                    return doi, None

            self.cache.set(cache_key, {'doi': None, 'error': "æœªæ‰¾åˆ°DOI"})
            return None, "æœªæ‰¾åˆ°DOI"
        except Exception as e:
            self.cache.set(cache_key, {'doi': None, 'error': f"CrossRefæŸ¥è¯¢å¤±è´¥: {str(e)}"})
            return None, f"CrossRefæŸ¥è¯¢å¤±è´¥: {str(e)}"

    def _fetch_arxiv(self, title: str) -> Tuple[Optional[str], Optional[str]]:
        """è¿”å› (PDFé“¾æ¥, é”™è¯¯ä¿¡æ¯)"""
        # æ£€æŸ¥ç¼“å­˜
        cache_key = f"arxiv:{hashlib.md5(title.encode()).hexdigest()}"
        cached = self.cache.get(cache_key)
        if cached:
            return cached.get('url'), cached.get('error')

        try:
            search_url = self.arxiv_api.format(title.replace(' ', '+'))
            resp = self.session.get(search_url, timeout=15)
            feed = feedparser.parse(resp.text)
            if feed.entries:
                for link in feed.entries[0].links:
                    if link.get('type') == 'application/pdf':
                        pdf_url = link.href.replace('http:', 'https:', 1)
                        self.cache.set(cache_key, {'url': pdf_url, 'error': None})
                        return pdf_url, None

            self.cache.set(cache_key, {'url': None, 'error': "æœªæ‰¾åˆ°arXivè®ºæ–‡"})
            return None, "æœªæ‰¾åˆ°arXivè®ºæ–‡"
        except Exception as e:
            self.cache.set(cache_key, {'url': None, 'error': f"arXivæ£€ç´¢å¤±è´¥: {str(e)}"})
            return None, f"arXivæ£€ç´¢å¤±è´¥: {str(e)}"

    def _fetch_scihub(self, search_param: str) -> Tuple[Optional[str], Optional[str]]:
        """è¿”å› (PDFé“¾æ¥, é”™è¯¯ä¿¡æ¯)"""
        # é¦–å…ˆå°è¯•å·²çŸ¥å·¥ä½œçš„é•œåƒ
        all_mirrors = self.active_mirrors.copy() + [m for m in self.scihub_urls if m not in self.active_mirrors]

        for base_url in all_mirrors:
            try:
                search_url = f"{base_url}/{search_param}"
                resp = self.session.get(search_url, timeout=20)
                if resp.status_code == 200:
                    soup = BeautifulSoup(resp.text, 'html.parser')
                    if pdf_url := self._parse_scihub_pdf_url(soup):
                        # æ·»åŠ åˆ°æ´»è·ƒé•œåƒåˆ—è¡¨
                        if base_url not in self.active_mirrors:
                            self.active_mirrors.append(base_url)

                        if pdf_url.startswith('//'):
                            return f"https:{pdf_url}", None
                        if pdf_url.startswith('/'):
                            return f"{base_url.rstrip('/')}{pdf_url}", None
                        if not pdf_url.startswith(('http://', 'https://')):
                            return f"{base_url.rstrip('/')}/{pdf_url.lstrip('/')}", None
                        return pdf_url, None
                elif resp.status_code == 403:
                    logging.warning(f"é•œåƒ {base_url} è§¦å‘åçˆ¬æœºåˆ¶")

            except Exception as e:
                logging.debug(f"é•œåƒ {base_url} è¯·æ±‚å¤±è´¥: {str(e)}")

            # çŸ­æš‚ç­‰å¾…åç»§ç»­å°è¯•ä¸‹ä¸€ä¸ªé•œåƒ
            time.sleep(random.uniform(0.5, 1.5))

        return None, "æ‰€æœ‰é•œåƒå‡å¤±è´¥"

    def _parse_scihub_pdf_url(self, soup: BeautifulSoup) -> Optional[str]:
        try:
            if button := soup.find('button', {'id': 'save'}):
                if onclick := button.get('onclick'):
                    return onclick.split("'")[1]
            if iframe := soup.find('iframe') or soup.find('embed'):
                return iframe['src']
            if pdf_link := soup.find('a', href=re.compile(r'.*\.pdf$')):
                return pdf_link['href']
        except Exception:
            return None
        return None

    def _fetch_scihub_selenium(self, title: str) -> Tuple[Optional[str], Optional[str]]:
        """ä½¿ç”¨Seleniumè·å–PDFé“¾æ¥"""
        try:
            downloader = SciHubDownloader(headless=True)
            pdf_url, error = downloader.fetch_pdf_url(title)
            downloader.close()
            return pdf_url, error
        except Exception as e:
            return None, f"Seleniumè·å–å¤±è´¥: {str(e)}"

            # çŸ­æš‚ç­‰å¾…åç»§ç»­å°è¯•ä¸‹ä¸€ä¸ªé•œåƒ
            time.sleep(random.uniform(0.5, 1.5))

        return None, "æ‰€æœ‰é•œåƒå‡å¤±è´¥"

    def _download_pdf(self, url: str, save_path: str) -> Tuple[bool, Optional[str]]:
        try:
            if url.startswith('//'):
                url = f"https:{url}"
            resp = self.session.get(url, stream=True, timeout=60)

            # æ£€æŸ¥æ˜¯å¦ä¸ºPDF
            content_type = resp.headers.get('Content-Type', '')
            if 'pdf' not in content_type.lower() and resp.content[:4] != b'%PDF':
                return False, "ä¸‹è½½å†…å®¹ä¸æ˜¯PDFæ–‡ä»¶"

            if resp.status_code == 200:
                total_size = int(resp.headers.get('Content-Length', 0))

                if total_size > 0:
                    bar_format = "{desc}: {percentage:3.0f}%|{bar}| {n_fmt}/{total_fmt}"
                    desc = f"ä¸‹è½½ {os.path.basename(save_path)[:20]}..."

                    with open(save_path, 'wb') as f, tqdm(
                            desc=desc,
                            total=total_size,
                            unit='B',
                            unit_scale=True,
                            bar_format=bar_format,
                            leave=False
                    ) as pbar:
                        for chunk in resp.iter_content(chunk_size=8192):
                            if chunk:
                                f.write(chunk)
                                pbar.update(len(chunk))
                else:
                    with open(save_path, 'wb') as f:
                        for chunk in resp.iter_content(chunk_size=8192):
                            if chunk:
                                f.write(chunk)

                # æ£€æŸ¥PDFæ–‡ä»¶å¤§å°
                if os.path.getsize(save_path) < 10000:  # æ–‡ä»¶å¤ªå°ï¼Œå¯èƒ½ä¸æ˜¯å®Œæ•´PDF
                    with open(save_path, 'rb') as f:
                        content = f.read(100)
                        if not content.startswith(b'%PDF'):
                            os.remove(save_path)  # åˆ é™¤æ— æ•ˆæ–‡ä»¶
                            return False, "ä¸‹è½½çš„æ–‡ä»¶ä¸æ˜¯æœ‰æ•ˆçš„PDF"

                return True, None
            return False, f"ä¸‹è½½å¤±è´¥ HTTP {resp.status_code}"
        except Exception as e:
            if os.path.exists(save_path):
                os.remove(save_path)  # æ¸…ç†éƒ¨åˆ†ä¸‹è½½çš„æ–‡ä»¶
            return False, f"ä¸‹è½½å¼‚å¸¸: {str(e)}"

    def download_by_title(self, title: str, save_path: str, retries=3) -> dict:
        """è¿”å›åŒ…å«å®Œæ•´çŠ¶æ€ä¿¡æ¯çš„å­—å…¸"""
        result = {
            'title': title,
            'status': 'å¤±è´¥',
            'method': None,
            'error': None,
            'save_path': save_path
        }

        logging.info(f"ğŸ” å¼€å§‹å¤„ç†: {title}")

        # å°è¯•ä¸åŒæ¥æº
        # ä½¿ç”¨å¯å»¶è¿Ÿæ‰§è¡Œçš„å‡½æ•°ï¼Œé˜²æ­¢ä¸å¿…è¦çš„APIè°ƒç”¨
        sources = [
            ('Sci-Hub (DOI)', functools.partial(self._try_doi_fetch, title)),
            ('arXiv', lambda: self._fetch_arxiv(title)),
            ('Sci-Hub (Selenium)', lambda: self._fetch_scihub_selenium(title)),
        ]

        for source_name, fetcher in sources:
            for attempt in range(retries):
                try:
                    logging.debug(f"å°è¯•æ¥æº: {source_name} (ç¬¬{attempt + 1}æ¬¡é‡è¯•)")
                    # è·å–PDFé“¾æ¥
                    pdf_url, error = fetcher()
                    if not pdf_url:
                        result['error'] = error
                        logging.warning(f"â“ {source_name} æœªæ‰¾åˆ°èµ„æº: {error}")
                        continue

                    # æ‰§è¡Œä¸‹è½½
                    logging.info(f"â¬‡ï¸ å°è¯•ä¸‹è½½: {pdf_url}")
                    success, dl_error = self._download_pdf(pdf_url, save_path)
                    if success:
                        result.update({
                            'status': 'æˆåŠŸ',
                            'method': source_name,
                            'error': None
                        })
                        logging.info(f"âœ… ä¸‹è½½æˆåŠŸ: {title} via {source_name}")
                        return result

                    result['error'] = dl_error
                    logging.warning(f"âš ï¸ ä¸‹è½½å¤±è´¥: {dl_error}")

                except Exception as e:
                    error_msg = f"å¼‚å¸¸: {str(e)}"
                    result['error'] = error_msg
                    logging.error(f"ğŸ”¥ å‘ç”Ÿå¼‚å¸¸: {error_msg}")

                # æ™ºèƒ½é€€é¿
                delay = min(2 ** attempt, 10)  # æŒ‡æ•°é€€é¿æœ€å¤§10ç§’
                time.sleep(delay)

        if result['status'] == 'å¤±è´¥':
            logging.error(f"âŒ æœ€ç»ˆå¤±è´¥: {title} | é”™è¯¯: {result.get('error', 'æœªçŸ¥é”™è¯¯')}")
        return result

    def _try_doi_fetch(self, title):
        """å…ˆè·å–DOIå†å°è¯•Sci-Hub"""
        doi, error = self._get_doi_from_title(title)
        if doi:
            return self._fetch_scihub(doi)
        return None, error or "æœªæ‰¾åˆ°DOI"

    def download_papers(self, titles: List[str], save_dir: str) -> Dict[str, int]:
        """ä½¿ç”¨çº¿ç¨‹æ± å¹¶è¡Œä¸‹è½½å¤šç¯‡è®ºæ–‡"""
        if not os.path.exists(save_dir):
            os.makedirs(save_dir, exist_ok=True)

        total = len(titles)
        stats = {'success': 0, 'fail': 0, 'skipped': 0}
        results = []

        # åˆå§‹åŒ–ç»“æœè®°å½•æ–‡ä»¶
        result_csv = 'download_results.csv'
        fieldnames = ['title', 'status', 'method', 'error', 'save_path']

        if not os.path.exists(result_csv):
            with open(result_csv, 'w', newline='', encoding='utf-8-sig') as f:
                writer = csv.DictWriter(f, fieldnames=fieldnames)
                writer.writeheader()
        else:
            with open(result_csv, 'w', encoding='utf-8-sig') as f:
                pass  # æ‰“å¼€æ–‡ä»¶å¹¶ç«‹å³å…³é—­ä»¥æ¸…ç©ºå†…å®¹

        # åˆ›å»ºç»“æœè®°å½•å¯¹è±¡
        result_file = open(result_csv, 'a', newline='', encoding='utf-8-sig')
        result_writer = csv.DictWriter(result_file, fieldnames=fieldnames)

        with tqdm(
                total=total,
                desc="ğŸ“¥ è®ºæ–‡ä¸‹è½½è¿›åº¦",
                unit="ç¯‡",
                bar_format="{l_bar}{bar}| {n_fmt}/{total_fmt} [å·²ç”¨:{elapsed}<å‰©ä½™:{remaining}]"
        ) as pbar:
            # åˆ›å»ºå¹¶æäº¤ä»»åŠ¡
            with ThreadPoolExecutor(max_workers=self.max_workers) as executor:
                future_to_title = {}

                for title in titles:
                    safe_title = re.sub(r'[\\/*?:"<>|]', '_', title)[:100]
                    save_path = os.path.join(save_dir, f"{safe_title}.pdf")

                    # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
                    if os.path.exists(save_path):
                        result = {
                            'title': title,
                            'status': 'è·³è¿‡',
                            'method': None,
                            'error': 'æ–‡ä»¶å·²å­˜åœ¨',
                            'save_path': save_path
                        }
                        result_writer.writerow(result)
                        stats['skipped'] += 1
                        pbar.update(1)
                        pbar.set_postfix(stats, refresh=True)
                        continue

                    # æäº¤ä»»åŠ¡
                    future = executor.submit(self.download_by_title, title, save_path)
                    future_to_title[future] = title

                # å¤„ç†å®Œæˆçš„ä»»åŠ¡
                for future in as_completed(future_to_title):
                    title = future_to_title[future]
                    try:
                        result = future.result()
                        if result['status'] == 'æˆåŠŸ':
                            stats['success'] += 1
                        else:
                            stats['fail'] += 1

                        # è®°å½•ç»“æœ
                        result_writer.writerow(result)
                        result_file.flush()  # ç«‹å³å†™å…¥ç£ç›˜

                    except Exception as e:
                        logging.error(f"å¤„ç†ä»»åŠ¡ç»“æœæ—¶å‡ºé”™: {str(e)}")
                        stats['fail'] += 1

                    pbar.update(1)
                    pbar.set_postfix(stats, refresh=True)

        # å…³é—­ç»“æœæ–‡ä»¶
        result_file.close()
        return stats


def read_titles_from_csv(file_path: str) -> List[str]:
    """ä»CSVæ–‡ä»¶è¯»å–è®ºæ–‡æ ‡é¢˜"""
    titles = []
    try:
        with open(file_path, 'r', encoding='utf-8-sig') as csvfile:
            reader = csv.DictReader(csvfile)
            titles = [row.get('Title', '').strip() for row in reader if row.get('Title', '').strip()]
        logging.info(f"æˆåŠŸè¯»å– {len(titles)} ç¯‡è®ºæ–‡æ ‡é¢˜")
    except Exception as e:
        logging.critical(f"ğŸ’¥ æ— æ³•è¯»å–è¾“å…¥æ–‡ä»¶: {str(e)}")
    return titles


def main(input_csv, save_dir, max_workers):
    # è®¾ç½®æ—¥å¿—
    setup_logging()

    # åˆå§‹åŒ–ä¸‹è½½å™¨ï¼Œè®¾ç½®å¹¶è¡Œæ•°
    downloader = PaperDownloader(max_workers=max_workers)  # è°ƒæ•´çº¿ç¨‹æ•°é‡

    # è¯»å–è¾“å…¥æ–‡ä»¶
    titles = read_titles_from_csv(input_csv)
    if not titles:
        logging.error("æ²¡æœ‰æ‰¾åˆ°è¦ä¸‹è½½çš„è®ºæ–‡æ ‡é¢˜!")
        return

    # æ‰§è¡Œä¸‹è½½
    stats = downloader.download_papers(titles, save_dir)

    # æœ€ç»ˆè¾“å‡º
    print(f"\nâœ… ä¸‹è½½å®Œæˆï¼æˆåŠŸ: {stats['success']} ç¯‡ | å¤±è´¥: {stats['fail']} ç¯‡ | è·³è¿‡: {stats['skipped']} ç¯‡")
    logging.info(f"æœ€ç»ˆç»Ÿè®¡ - {stats}")
    logging.info("ğŸ ç¨‹åºè¿è¡Œç»“æŸ")


if __name__ == "__main__":
    input_csv = 'wos_results.csv'
    save_dir = 'pulsed_laser_deposition'
    main(input_csv, save_dir, 5)
